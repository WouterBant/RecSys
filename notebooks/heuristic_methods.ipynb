{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/teamspace/studios/this_studio/clean/RecSys/code\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd clean/RecSys/code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class ArgsVal:\n",
    "    dataset: str = \"demo\"\n",
    "    T: int = 4  # number of previous articles to consider in the prompt\n",
    "    datafraction: float = 1.0  # how much of entire dataset to use\n",
    "    model: str = \"CG\"  # just placeholder conditional generation\n",
    "    backbone: str = \"google/mt5-small\"\n",
    "    tokenizer: str = \"google/mt5-small\"\n",
    "    evaltrain: bool = False  # allows for evaluation on training set\n",
    "\n",
    "args = ArgsVal()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.collators import CollatorValidation\n",
    "from transformers import AutoTokenizer\n",
    "from data.dataloader import EkstraBladetDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from utils.prompt_templates import create_prompt_titles\n",
    "from utils.metrics import MetricsEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"google/mt5-small\")\n",
    "collator = CollatorValidation(tokenizer)\n",
    "data = EkstraBladetDataset(args, create_prompt_titles, split=\"validation\")\n",
    "dl = DataLoader(data, batch_size=1, collate_fn=collator, num_workers=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "  0%|          | 0/25356 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "100%|██████████| 25356/25356 [01:05<00:00, 386.32it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'AUC': 0.6916642188873707,\n",
       "             'ndcg': 0.5322142751724712,\n",
       "             'ndcg@5': 0.44624063306448813,\n",
       "             'mrr': 0.38956835789108085,\n",
       "             'mrr@5': 0.35576786559392687,\n",
       "             'precision@1': 0.1690724088973024,\n",
       "             'recall@5': 0.7237241678498186,\n",
       "             'hit_ratio@5': 0.7242467266130305,\n",
       "             'diversity@1': 0.49451806278592836,\n",
       "             'diversity@2': 0.4913827102066572,\n",
       "             'diversity@3': 0.48883893358574376,\n",
       "             'diversity@4': 0.4880205868433507,\n",
       "             'diversity@5': 0.48997475942578106,\n",
       "             'intra_list_diversity@1': 1.0,\n",
       "             'intra_list_diversity@2': 0.8861019088184257,\n",
       "             'intra_list_diversity@3': 0.8097623179260819,\n",
       "             'intra_list_diversity@4': 0.745149077141505,\n",
       "             'intra_list_diversity@5': 0.6928616501024476})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_evaluator = MetricsEvaluator()\n",
    "results = defaultdict(int)\n",
    "total = 0\n",
    "for ex in tqdm(iter(dl)):\n",
    "    pubtimes = ex[\"publish_time\"]\n",
    "    last_clicked_article = pubtimes[3]\n",
    "    inview_articles_times = pubtimes[4:]\n",
    "\n",
    "    # Calculate the absolute difference in seconds between each timestamp and the reference timestamp\n",
    "    differences = [abs((ts - last_clicked_article).total_seconds()) for ts in inview_articles_times]\n",
    "\n",
    "    # Normalize the differences to convert them into scores (higher score for closer timestamps)\n",
    "    # We'll use an inverse proportionality for scoring\n",
    "    max_diff = max(differences)\n",
    "    scores = [(max_diff - diff) for diff in differences]\n",
    "\n",
    "    # Normalize scores to be between 0 and 1 (optional, for better interpretability)\n",
    "    min_score, max_score = min(scores), max(scores)\n",
    "    scores = [(score - min_score) / (max_score - min_score) for score in scores]\n",
    "\n",
    "    metrics = metrics_evaluator.compute_metrics({\n",
    "        'scores': np.array(scores),\n",
    "        'labels': np.array(ex[\"targets\"]),\n",
    "        'categories': np.array(ex[\"categories\"]),\n",
    "    })\n",
    "\n",
    "    # Sum the metrics\n",
    "    for key in metrics:\n",
    "        results[key] += metrics[key]\n",
    "    total += 1\n",
    "\n",
    "# Take the average of the metrics over the dataset\n",
    "for key in results:\n",
    "    results[key] /= total\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "  0%|          | 0/25356 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "  1%|          | 131/25356 [00:00<01:35, 263.55it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25356/25356 [00:50<00:00, 501.71it/s]\n"
     ]
    }
   ],
   "source": [
    "metrics_evaluator = MetricsEvaluator()\n",
    "results = defaultdict(int)\n",
    "total = 0\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "for ex in tqdm(iter(dl)):\n",
    "    pubtimes = ex[\"publish_time\"]\n",
    "    random_scores = np.random.random(len(pubtimes)-4)\n",
    "\n",
    "    metrics = metrics_evaluator.compute_metrics({\n",
    "        'scores': np.array(random_scores),\n",
    "        'labels': np.array(ex[\"targets\"]),\n",
    "        'categories': np.array(ex[\"categories\"]),\n",
    "    })\n",
    "\n",
    "    # Sum the metrics\n",
    "    for key in metrics:\n",
    "        results[key] += metrics[key]\n",
    "    total += 1\n",
    "\n",
    "# Take the average of the metrics over the dataset\n",
    "for key in results:\n",
    "    results[key] /= total\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "  0%|          | 0/25356 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "  0%|          | 47/25356 [00:00<02:36, 161.83it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25356/25356 [00:50<00:00, 497.24it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'AUC': 0.5968676248598606,\n",
       "             'ndcg': 0.49218359095923137,\n",
       "             'ndcg@5': 0.37856899367455715,\n",
       "             'mrr': 0.3427683923720312,\n",
       "             'mrr@5': 0.30025569227530546,\n",
       "             'precision@1': 0.14434453383814483,\n",
       "             'recall@5': 0.6199584582215912,\n",
       "             'hit_ratio@5': 0.6204843035179051,\n",
       "             'diversity@1': 0.02539832781195772,\n",
       "             'diversity@2': 0.058625177472787504,\n",
       "             'diversity@3': 0.10931009097123734,\n",
       "             'diversity@4': 0.1708274175737498,\n",
       "             'diversity@5': 0.2369774412367776,\n",
       "             'intra_list_diversity@1': 1.0,\n",
       "             'intra_list_diversity@2': 0.6913748225272125,\n",
       "             'intra_list_diversity@3': 0.5985434085292004,\n",
       "             'intra_list_diversity@4': 0.5592364726297523,\n",
       "             'intra_list_diversity@5': 0.5407556396908079})"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_evaluator = MetricsEvaluator()\n",
    "results = defaultdict(int)\n",
    "total = 0\n",
    "for ex in tqdm(iter(dl)):\n",
    "    categories = ex[\"categories\"]\n",
    "    last_clicked_article = categories[:4]\n",
    "    inview_articles_cats = categories[4:]\n",
    "\n",
    "    category_counts = Counter(last_clicked_article)\n",
    "    m = max(category_counts.values())\n",
    "    scores = [category_counts[category]/m for category in inview_articles_cats]\n",
    "\n",
    "    metrics = metrics_evaluator.compute_metrics({\n",
    "        'scores': np.array(scores),\n",
    "        'labels': np.array(ex[\"targets\"]),\n",
    "        'categories': np.array(ex[\"categories\"]),\n",
    "    })\n",
    "\n",
    "    # Sum the metrics\n",
    "    for key in metrics:\n",
    "        results[key] += metrics[key]\n",
    "    total += 1\n",
    "\n",
    "# Take the average of the metrics over the dataset\n",
    "for key in results:\n",
    "    results[key] /= total\n",
    "results"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

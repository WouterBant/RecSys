{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "import random\n",
    "from prompt_templates import create_prompt_subtitles\n",
    "from transformers import MT5ForConditionalGeneration\n",
    "import argparse\n",
    "from datetime import datetime\n",
    "from models import get_model\n",
    "from dataloader import get_loader\n",
    "import torch\n",
    "from torch import nn\n",
    "from evaluate import evaluate\n",
    "import copy\n",
    "import wandb\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import json\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from transformers import AutoTokenizer\n",
    "from scheduler import CosineWarmupScheduler\n",
    "from utils import compute_rank_loss\n",
    "from torch.cuda.amp import GradScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /home/colin/miniconda3/envs/recsys/lib/python3.11/site-packages (from scikit-learn) (1.26.4)\n",
      "Collecting scipy>=1.6.0 (from scikit-learn)\n",
      "  Downloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading scikit_learn-1.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Downloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
      "Successfully installed joblib-1.4.2 scikit-learn-1.5.0 scipy-1.13.1 threadpoolctl-3.5.0\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "/home/colin/miniconda3/envs/recsys/lib/python3.11/site-packages/transformers/convert_slow_tokenizer.py:560: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"google/mt5-small\")\n",
    "model = MT5ForConditionalGeneration.from_pretrained(\"google/mt5-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.base_model.load_state_dict(torch.load('checkpoints/model_0.0001.pth',map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class b:\n",
    "    dataset = \"demo\"\n",
    "    batch_size = 3\n",
    "    num_workers = 4\n",
    "    titles = True\n",
    "    T=4\n",
    "    datafraction=0.001\n",
    "\n",
    "b = b()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = get_loader(b, 'train', tokenizer, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader2 = get_loader(b, 'train', tokenizer, debug=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "/home/colin/miniconda3/envs/recsys/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:3946: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/colin/miniconda3/envs/recsys/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:3946: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/colin/miniconda3/envs/recsys/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:3946: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/colin/miniconda3/envs/recsys/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:3946: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "out1= next(iter(data_loader2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['pos_input_ids', 'pos_attention_mask', 'neg_input_ids', 'neg_attention_mask', 'pos_labels', 'neg_labels', 'decoder_start'])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out1.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 113]),\n",
       " torch.Size([3, 113]),\n",
       " torch.Size([3, 110]),\n",
       " torch.Size([3, 2]),\n",
       " torch.Size([3, 110]))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out1['pos_input_ids'].shape, out1['pos_attention_mask'].shape, out1['neg_input_ids'].shape, out1['pos_labels'].shape, out1['neg_attention_mask'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 5])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out1[\"decoder_start\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "En bruger har for nylig læst artikler: Holger Runes showkamp i Royal Arena er aflyst, Putins 'vasketrick': Nu reagerer de, Oppositionsleder i nødråb: - Han sælger vores land, Mikkel Kessler sælger huset, vil brugeren læse artiklen Netbank ramt af nedbrud? (ja/nej)</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "En bruger har for nylig læst artikler: Holger Runes showkamp i Royal Arena er aflyst, Putins 'vasketrick': Nu reagerer de, Oppositionsleder i nødråb: - Han sælger vores land, Mikkel Kessler sælger huset, vil brugeren læse artiklen Ukraine klar til modangreb: Her kan de presse Putin? (ja/nej)</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "En bruger har for nylig læst artikler: Midt i skole-fejring: Rykker ud til knivstikkeri, Sender stikpille til dommer: - Han læser aftrykkene forkert, Knivstikkeri i Fælledparken: En anholdt og sigtet, Total ydmygelse: City smadrer Real Madrid og er klar til CL-, vil brugeren læse artiklen Kæmpe finale venter: Her er Holgers modstander? (ja/nej)</s>\n",
      "En bruger har for nylig læst artikler: Midt i skole-fejring: Rykker ud til knivstikkeri, Sender stikpille til dommer: - Han læser aftrykkene forkert, Knivstikkeri i Fælledparken: En anholdt og sigtet, Total ydmygelse: City smadrer Real Madrid og er klar til CL-, vil brugeren læse artiklen Lufthavnskaos: Her er dine rettigheder? (ja/nej)</s>\n",
      "En bruger har for nylig læst artikler: Midt i skole-fejring: Rykker ud til knivstikkeri, Sender stikpille til dommer: - Han læser aftrykkene forkert, Knivstikkeri i Fælledparken: En anholdt og sigtet, Total ydmygelse: City smadrer Real Madrid og er klar til CL-, vil brugeren læse artiklen - Hvordan skal jeg svare på det?? (ja/nej)</s><pad><pad><pad>\n",
      "En bruger har for nylig læst artikler: Midt i skole-fejring: Rykker ud til knivstikkeri, Sender stikpille til dommer: - Han læser aftrykkene forkert, Knivstikkeri i Fælledparken: En anholdt og sigtet, Total ydmygelse: City smadrer Real Madrid og er klar til CL-, vil brugeren læse artiklen Maiken har tabu-fantasier? (ja/nej)</s><pad><pad><pad>\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print(tokenizer.decode(out1['pos_input_ids'][i].tolist()))\n",
    "    print(tokenizer.decode(out1['neg_input_ids'][i].tolist()))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([432, 259, 275, 3810, 1], [432, 1], [3810, 1])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer.encode(\"ja / nej\"), tokenizer.encode(\"ja\"), tokenizer.encode(\"nej\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_outputs = model(\n",
    "    input_ids=out1[\"pos_input_ids\"], \n",
    "    attention_mask=out1[\"pos_attention_mask\"],\n",
    "    decoder_input_ids=out1[\"decoder_start\"][:,],\n",
    ")\n",
    "neg_outputs = model(\n",
    "    input_ids=out1[\"neg_input_ids\"], \n",
    "    attention_mask=out1[\"neg_attention_mask\"],\n",
    "    decoder_input_ids=out1[\"decoder_start\"],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== pos examples ==\n",
      "ja: tensor([4.3720, 4.0491, 4.2663], grad_fn=<SelectBackward0>)\n",
      "nej: tensor([5.5062, 5.7165, 5.4130], grad_fn=<SelectBackward0>)\n",
      "all other tokens (mean): tensor([-22.7337, -22.8223, -22.8444], grad_fn=<MeanBackward1>)\n",
      "all other tokens (std): tensor([0.4487, 0.4915, 0.4547], grad_fn=<StdBackward0>)\n",
      "== pos examples ==\n",
      "ja: tensor([5.0834, 4.0282, 4.1644], grad_fn=<SelectBackward0>)\n",
      "nej: tensor([4.3363, 5.6186, 5.9130], grad_fn=<SelectBackward0>)\n",
      "all other tokens (mean): tensor([-22.6058, -22.9179, -22.8858], grad_fn=<MeanBackward1>)\n",
      "all other tokens (std): tensor([0.4334, 0.4995, 0.4922], grad_fn=<StdBackward0>)\n"
     ]
    }
   ],
   "source": [
    "ja = tokenizer.encode('ja')[0]\n",
    "nej = tokenizer.encode('nej')[0]\n",
    "\n",
    "other_logit_idxs = [i for i in range(pos_outputs.logits.shape[2])]\n",
    "other_logit_idxs.remove(ja)\n",
    "other_logit_idxs.remove(nej)\n",
    "\n",
    "print('== pos examples ==')\n",
    "print('ja:',pos_outputs.logits[:,0,ja])\n",
    "print('nej:',pos_outputs.logits[:,0,nej])\n",
    "other_logits_pos = pos_outputs.logits[:,0,other_logit_idxs]\n",
    "print('all other tokens (mean):',torch.mean(other_logits_pos,dim=-1))\n",
    "print('all other tokens (std):',torch.std(other_logits_pos,dim=-1))\n",
    "\n",
    "\n",
    "\n",
    "print('== pos examples ==')\n",
    "print('ja:',neg_outputs.logits[:,0,ja])\n",
    "print('nej:',neg_outputs.logits[:,0,nej])\n",
    "other_logits_neg = neg_outputs.logits[:,0,other_logit_idxs]\n",
    "print('all other tokens (mean):',torch.mean(other_logits_neg,dim=-1))\n",
    "print('all other tokens (std):',torch.std(other_logits_neg,dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2, 250112])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_outputs.logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['pos_input_ids', 'pos_attention_mask', 'neg_input_ids', 'neg_attention_mask', 'pos_labels', 'neg_labels', 'empt'])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out1.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "En bruger har for nylig læst artikler: Tennis-etteren Novak Djokovic havde flotte ord om Holger Rune på pressemødet efter de to tennisstjerners dyst i Rom og fortæller, at det er ham, der skal have råd fra Rune og ikke omvendt, Rafael Nadal har indkaldt til pressemøde torsdag. Ifølge AS og Marca vil han annoncere French Open-afbud, Rygtemøllen arbejder på højtryk, efter Belarus' præsident, Alexander Lukasjenko, ikke er blevet set offentligt i en uge, Luksuslivet er slut. Bruger penge på Easyjet, sandkage og cola, vil brugeren læse artiklen - Hvis nogen tror, at vi lægger os fladt ned mod FC Nordsjælland, så må det være personer, der ikke er involveret i elitesport, siger en vred Carsten V. Jensen? (ja/nej)</s>\n",
      "En bruger har for nylig læst artikler: Tennis-etteren Novak Djokovic havde flotte ord om Holger Rune på pressemødet efter de to tennisstjerners dyst i Rom og fortæller, at det er ham, der skal have råd fra Rune og ikke omvendt, Rafael Nadal har indkaldt til pressemøde torsdag. Ifølge AS og Marca vil han annoncere French Open-afbud, Rygtemøllen arbejder på højtryk, efter Belarus' præsident, Alexander Lukasjenko, ikke er blevet set offentligt i en uge, Luksuslivet er slut. Bruger penge på Easyjet, sandkage og cola, vil brugeren læse artiklen Aktien er faldet med knap 30 procent onsdag? (ja/nej)</s>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(out1['pos_input_ids'][0].tolist()))\n",
    "print(tokenizer.decode(out1['neg_input_ids'][0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"En bruger har for nylig læst artikler: Tennis-etteren Novak Djokovic havde flotte ord om Holger Rune på pressemødet efter de to tennisstjerners dyst i Rom og fortæller, at det er ham, der skal have råd fra Rune og ikke omvendt, Rafael Nadal har indkaldt til pressemøde torsdag. Ifølge AS og Marca vil han annoncere French Open-afbud, Rygtemøllen arbejder på højtryk, efter Belarus' præsident, Alexander Lukasjenko, ikke er blevet set offentligt i en uge, Luksuslivet er slut. Bruger penge på Easyjet, sandkage og cola, vil brugeren læse artiklen Aktien er faldet med knap 30 procent onsdag? (ja/nej)</s>\""
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[124393, 1]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "/home/wouter/miniconda3/envs/RecSys/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:3946: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/wouter/miniconda3/envs/RecSys/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:3946: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "/home/wouter/miniconda3/envs/RecSys/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:3946: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/wouter/miniconda3/envs/RecSys/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:3946: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "/home/wouter/miniconda3/envs/RecSys/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:3946: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/wouter/miniconda3/envs/RecSys/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:3946: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/wouter/miniconda3/envs/RecSys/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:3946: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "/home/wouter/miniconda3/envs/RecSys/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:3946: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'dict' object is not an iterator",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m out1, out2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(data_loader2)), \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata_loader2\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'dict' object is not an iterator"
     ]
    }
   ],
   "source": [
    "out1, out2 = next(iter(data_loader2)), next(next(iter(data_loader2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RecSys",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

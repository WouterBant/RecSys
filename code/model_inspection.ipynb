{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/colin/miniconda3/envs/recsys/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "import random\n",
    "from prompt_templates import create_prompt_subtitles\n",
    "from transformers import MT5ForConditionalGeneration\n",
    "import argparse\n",
    "from datetime import datetime\n",
    "from models import get_model\n",
    "from dataloader import get_loader\n",
    "import torch\n",
    "from torch import nn\n",
    "from evaluate import evaluate\n",
    "import copy\n",
    "import wandb\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import json\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from transformers import AutoTokenizer\n",
    "from scheduler import CosineWarmupScheduler\n",
    "from utils import compute_rank_loss\n",
    "from torch.cuda.amp import GradScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MT5ForConditionalGeneration(\n",
       "  (shared): Embedding(250112, 512)\n",
       "  (encoder): MT5Stack(\n",
       "    (embed_tokens): Embedding(250112, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): MT5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): MT5LayerSelfAttention(\n",
       "            (SelfAttention): MT5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 6)\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): MT5LayerFF(\n",
       "            (DenseReluDense): MT5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-7): 7 x MT5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): MT5LayerSelfAttention(\n",
       "            (SelfAttention): MT5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): MT5LayerFF(\n",
       "            (DenseReluDense): MT5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): MT5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): MT5Stack(\n",
       "    (embed_tokens): Embedding(250112, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): MT5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): MT5LayerSelfAttention(\n",
       "            (SelfAttention): MT5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 6)\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): MT5LayerCrossAttention(\n",
       "            (EncDecAttention): MT5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): MT5LayerFF(\n",
       "            (DenseReluDense): MT5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-7): 7 x MT5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): MT5LayerSelfAttention(\n",
       "            (SelfAttention): MT5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): MT5LayerCrossAttention(\n",
       "            (EncDecAttention): MT5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): MT5LayerFF(\n",
       "            (DenseReluDense): MT5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): MT5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=250112, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"google/mt5-small\")\n",
    "model = MT5ForConditionalGeneration.from_pretrained(\"google/mt5-small\")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.base_model.load_state_dict(torch.load('checkpoints/model_0.0001.pth',map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class b:\n",
    "    dataset = \"demo\"\n",
    "    batch_size = 3\n",
    "    num_workers = 4\n",
    "    titles = True\n",
    "    T=4\n",
    "    datafraction=0.001\n",
    "\n",
    "b = b()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = get_loader(b, 'train', tokenizer, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader2 = get_loader(b, 'train', tokenizer, debug=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "/home/colin/miniconda3/envs/recsys/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:3946: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/colin/miniconda3/envs/recsys/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:3946: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/colin/miniconda3/envs/recsys/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:3946: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/colin/miniconda3/envs/recsys/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:3946: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "out1= next(iter(data_loader2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['pos_input_ids', 'pos_attention_mask', 'neg_input_ids', 'neg_attention_mask', 'pos_labels', 'neg_labels', 'decoder_start'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out1.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 110]),\n",
       " torch.Size([3, 110]),\n",
       " torch.Size([3, 110]),\n",
       " torch.Size([3, 2]),\n",
       " torch.Size([3, 110]),\n",
       " torch.Size([3, 5]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out1['pos_input_ids'].shape, out1['pos_attention_mask'].shape, out1['neg_input_ids'].shape, out1['pos_labels'].shape, out1['neg_attention_mask'].shape, out1[\"decoder_start\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "En bruger har for nylig læst artikler: Midt i skole-fejring: Rykker ud til knivstikkeri, Sender stikpille til dommer: - Han læser aftrykkene forkert, Knivstikkeri i Fælledparken: En anholdt og sigtet, Total ydmygelse: City smadrer Real Madrid og er klar til CL-, vil brugeren læse artiklen - Hvordan skal jeg svare på det?? (ja/nej)</s>\n",
      "En bruger har for nylig læst artikler: Midt i skole-fejring: Rykker ud til knivstikkeri, Sender stikpille til dommer: - Han læser aftrykkene forkert, Knivstikkeri i Fælledparken: En anholdt og sigtet, Total ydmygelse: City smadrer Real Madrid og er klar til CL-, vil brugeren læse artiklen Lufthavnskaos: Her er dine rettigheder? (ja/nej)</s>\n",
      "En bruger har for nylig læst artikler: Bortført pige fundet efter seks år: Genkendt i Netflix-serie, Træt af spioner: For mange udnytter det, Så klappede fælden: Techsvindler skal bag tremmer, Derfor kom Kjær ikke ind: Var for langsom, vil brugeren læse artiklen Kom med på fremtidens McDonald's? (ja/nej)</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "En bruger har for nylig læst artikler: Bortført pige fundet efter seks år: Genkendt i Netflix-serie, Træt af spioner: For mange udnytter det, Så klappede fælden: Techsvindler skal bag tremmer, Derfor kom Kjær ikke ind: Var for langsom, vil brugeren læse artiklen Markant Tour-omvæltning: En stor dansk gave i Paris? (ja/nej)</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "En bruger har for nylig læst artikler: Smitten hærger i Giroen: - Et kæmpe chok, Blev afbrudt under sex: Gav forbipasserende lussing, WHO advarer: - Drop det!, Så er den gal igen: Syv anholdt for ulovligt arbejde, vil brugeren læse artiklen Lokale supermarkeder tilbyder måske studenter ulovlige øl? (ja/nej)</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "En bruger har for nylig læst artikler: Smitten hærger i Giroen: - Et kæmpe chok, Blev afbrudt under sex: Gav forbipasserende lussing, WHO advarer: - Drop det!, Så er den gal igen: Syv anholdt for ulovligt arbejde, vil brugeren læse artiklen Engell: Her er vinderne - og taberne? (ja/nej)</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print(tokenizer.decode(out1['pos_input_ids'][i].tolist()))\n",
    "    print(tokenizer.decode(out1['neg_input_ids'][i].tolist()))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([432, 259, 275, 3810, 1], [432, 1], [3810, 1])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"ja / nej\"), tokenizer.encode(\"ja\"), tokenizer.encode(\"nej\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_outputs = model(\n",
    "    input_ids=out1[\"pos_input_ids\"], \n",
    "    attention_mask=out1[\"pos_attention_mask\"],\n",
    "    decoder_input_ids=out1[\"decoder_start\"],\n",
    ")\n",
    "neg_outputs = model(\n",
    "    input_ids=out1[\"neg_input_ids\"], \n",
    "    attention_mask=out1[\"neg_attention_mask\"],\n",
    "    decoder_input_ids=out1[\"decoder_start\"],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== pos examples ==\n",
      "ja: tensor([4.2663, 4.7666, 4.8914], grad_fn=<SelectBackward0>)\n",
      "nej: tensor([5.4130, 5.3510, 4.7950], grad_fn=<SelectBackward0>)\n",
      "all other tokens (mean): tensor([-22.8444, -22.8865, -22.5977], grad_fn=<MeanBackward1>)\n",
      "all other tokens (std): tensor([0.4547, 0.4488, 0.4111], grad_fn=<StdBackward0>)\n",
      "== pos examples ==\n",
      "ja: tensor([4.0282, 4.5596, 4.9551], grad_fn=<SelectBackward0>)\n",
      "nej: tensor([5.6186, 5.4770, 5.0416], grad_fn=<SelectBackward0>)\n",
      "all other tokens (mean): tensor([-22.9179, -22.7150, -22.6757], grad_fn=<MeanBackward1>)\n",
      "all other tokens (std): tensor([0.4995, 0.4571, 0.4148], grad_fn=<StdBackward0>)\n"
     ]
    }
   ],
   "source": [
    "ja = tokenizer.encode('ja')[0]\n",
    "nej = tokenizer.encode('nej')[0]\n",
    "\n",
    "other_logit_idxs = [i for i in range(pos_outputs.logits.shape[2])]\n",
    "other_logit_idxs.remove(ja)\n",
    "other_logit_idxs.remove(nej)\n",
    "\n",
    "print('== pos examples ==')\n",
    "print('ja:',pos_outputs.logits[:,0,ja])\n",
    "print('nej:',pos_outputs.logits[:,0,nej])\n",
    "other_logits_pos = pos_outputs.logits[:,0,other_logit_idxs]\n",
    "print('all other tokens (mean):',torch.mean(other_logits_pos,dim=-1))\n",
    "print('all other tokens (std):',torch.std(other_logits_pos,dim=-1))\n",
    "\n",
    "\n",
    "\n",
    "print('== pos examples ==')\n",
    "print('ja:',neg_outputs.logits[:,0,ja])\n",
    "print('nej:',neg_outputs.logits[:,0,nej])\n",
    "other_logits_neg = neg_outputs.logits[:,0,other_logit_idxs]\n",
    "print('all other tokens (mean):',torch.mean(other_logits_neg,dim=-1))\n",
    "print('all other tokens (std):',torch.std(other_logits_neg,dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 110])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out1[\"pos_input_ids\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel, utils\n",
    "from bertviz import model_view\n",
    "utils.logging.set_verbosity_error()  # Suppress standard warnings\n",
    "\n",
    "# model_name = \"google/mt5-small\"  # Find popular HuggingFace models here: https://huggingface.co/models\n",
    "# input_text = \"The cat sat on the mat\"  \n",
    "# model = MT5ForConditionalGeneration.from_pretrained(model_name, output_attentions=True)  # Configure model to return attention values\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# inputs = tokenizer.encode(input_text, return_tensors='pt')  # Tokenize input text\n",
    "outputs = model(input_ids=out1[\"pos_input_ids\"][0].unsqueeze(0), attention_mask=out1[\"pos_attention_mask\"][0].unsqueeze(0), decoder_input_ids=out1[\"decoder_start\"][0].unsqueeze(0))  # Run model\n",
    "attention = outputs[-1]  # Retrieve attention from model outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 250112])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer.convert_ids_to_tokens(out1[\"pos_input_ids\"][0])  # Convert input ids to token strings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The attention tensor does not have the correct number of dimensions. Make sure you set output_attentions=True when initializing your model.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel_view\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Display model view\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/recsys/lib/python3.11/site-packages/bertviz/model_view.py:67\u001b[0m, in \u001b[0;36mmodel_view\u001b[0;34m(attention, tokens, sentence_b_start, prettify_tokens, display_mode, encoder_attention, decoder_attention, cross_attention, encoder_tokens, decoder_tokens, include_layers, include_heads, html_action)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m include_heads \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     66\u001b[0m     include_heads \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mrange\u001b[39m(n_heads))\n\u001b[0;32m---> 67\u001b[0m attention \u001b[38;5;241m=\u001b[39m \u001b[43mformat_attention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude_layers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude_heads\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sentence_b_start \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m     attn_data\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m     70\u001b[0m         {\n\u001b[1;32m     71\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     75\u001b[0m         }\n\u001b[1;32m     76\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/recsys/lib/python3.11/site-packages/bertviz/util.py:11\u001b[0m, in \u001b[0;36mformat_attention\u001b[0;34m(attention, layers, heads)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer_attention \u001b[38;5;129;01min\u001b[39;00m attention:\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m# 1 x num_heads x seq_len x seq_len\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(layer_attention\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m4\u001b[39m:\n\u001b[0;32m---> 11\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe attention tensor does not have the correct number of dimensions. Make sure you set \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     12\u001b[0m                          \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_attentions=True when initializing your model.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     13\u001b[0m     layer_attention \u001b[38;5;241m=\u001b[39m layer_attention\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m heads:\n",
      "\u001b[0;31mValueError\u001b[0m: The attention tensor does not have the correct number of dimensions. Make sure you set output_attentions=True when initializing your model."
     ]
    }
   ],
   "source": [
    "model_view(attention, tokens)  # Display model view"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RecSys",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
